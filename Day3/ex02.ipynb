{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDoubleSafe(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except ValueError:\n",
    "        return str(v) #if it is not a float type return as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and convert the data\n",
    "census_raw = sc.textFile(\"../Data/adult.raw\", 4).map(lambda x:  x.split(\", \"))\n",
    "census_raw = census_raw.map(lambda row:  [toDoubleSafe(x) for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the RDD to DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+---------+--------+--------------+------+------------------+--------------+-----------------+-----+-------------+------+----------------+\n",
      "| age|capital_gain|capital_loss|education|  fnlwgt|hours_per_week|income|    marital_status|native_country|       occupation| race| relationship|   sex|       workclass|\n",
      "+----+------------+------------+---------+--------+--------------+------+------------------+--------------+-----------------+-----+-------------+------+----------------+\n",
      "|39.0|      2174.0|         0.0|Bachelors| 77516.0|          40.0| <=50K|     Never-married| United-States|     Adm-clerical|White|Not-in-family|  Male|       State-gov|\n",
      "|50.0|         0.0|         0.0|Bachelors| 83311.0|          13.0| <=50K|Married-civ-spouse| United-States|  Exec-managerial|White|      Husband|  Male|Self-emp-not-inc|\n",
      "|38.0|         0.0|         0.0|  HS-grad|215646.0|          40.0| <=50K|          Divorced| United-States|Handlers-cleaners|White|Not-in-family|  Male|         Private|\n",
      "|53.0|         0.0|         0.0|     11th|234721.0|          40.0| <=50K|Married-civ-spouse| United-States|Handlers-cleaners|Black|      Husband|  Male|         Private|\n",
      "|28.0|         0.0|         0.0|Bachelors|338409.0|          40.0| <=50K|Married-civ-spouse|          Cuba|   Prof-specialty|Black|         Wife|Female|         Private|\n",
      "+----+------------+------------+---------+--------+--------------+------+------------------+--------------+-----------------+-----+-------------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "adultschema = StructType([\n",
    "    StructField(\"age\",DoubleType(),True),\n",
    "    StructField(\"capital_gain\",DoubleType(),True),\n",
    "    StructField(\"capital_loss\",DoubleType(),True),\n",
    "    StructField(\"education\",StringType(),True),\n",
    "    StructField(\"fnlwgt\",DoubleType(),True),\n",
    "    StructField(\"hours_per_week\",DoubleType(),True),\n",
    "    StructField(\"income\",StringType(),True),\n",
    "    StructField(\"marital_status\",StringType(),True),\n",
    "    StructField(\"native_country\",StringType(),True),\n",
    "    StructField(\"occupation\",StringType(),True),\n",
    "    StructField(\"race\",StringType(),True),\n",
    "    StructField(\"relationship\",StringType(),True),\n",
    "    StructField(\"sex\",StringType(),True),\n",
    "    StructField(\"workclass\",StringType(),True),\n",
    "])\n",
    "\n",
    "# Create a dataframe.\n",
    "from pyspark.sql import Row\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
    "           \"hours_per_week\", \"native_country\", \"income\"]\n",
    "dfraw = ss.createDataFrame(census_raw.map(lambda row: Row(**{x[0]: x[1] for x in zip(columns, row)})), \\\n",
    "                           adultschema)\n",
    "dfraw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39.0, 'State-gov', 77516.0, 'Bachelors', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', 2174.0, 0.0, 40.0, 'United-States', '<=50K']]\n",
      "[<zip object at 0x121a4a410>]\n",
      "[{'age': 39.0, 'workclass': 'State-gov', 'fnlwgt': 77516.0, 'education': 'Bachelors', 'marital_status': 'Never-married', 'occupation': 'Adm-clerical', 'relationship': 'Not-in-family', 'race': 'White', 'sex': 'Male', 'capital_gain': 2174.0, 'capital_loss': 0.0, 'hours_per_week': 40.0, 'native_country': 'United-States', 'income': '<=50K'}]\n",
      "[Row(age=39.0, capital_gain=2174.0, capital_loss=0.0, education='Bachelors', fnlwgt=77516.0, hours_per_week=40.0, income='<=50K', marital_status='Never-married', native_country='United-States', occupation='Adm-clerical', race='White', relationship='Not-in-family', sex='Male', workclass='State-gov')]\n"
     ]
    }
   ],
   "source": [
    "# Original:\n",
    "print(census_raw.take(1))\n",
    "\n",
    "# Returns a list of tuples.\n",
    "# zip() : returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.\n",
    "print(census_raw.map(lambda x :  zip(columns, x)).take(1))\n",
    "\n",
    "# Transform a list into a list with keyword arguments.\n",
    "print(census_raw.map(lambda x : {x[0]: x[1] for x in zip(columns, x)}).take(1))\n",
    "\n",
    "# Transform into a row using variable with keywords.\n",
    "# As this is keyworded, createDataFrame() will match the column name and apply the defined schema.\n",
    "print(census_raw.map(lambda x : Row(**{x[0]: x[1] for x in zip(columns, x)})).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       workclass|count|\n",
      "+----------------+-----+\n",
      "|         Private|33906|\n",
      "|Self-emp-not-inc| 3862|\n",
      "|       Local-gov| 3136|\n",
      "|               ?| 2799|\n",
      "|       State-gov| 1981|\n",
      "|    Self-emp-inc| 1695|\n",
      "|     Federal-gov| 1432|\n",
      "|     Without-pay|   21|\n",
      "|    Never-worked|   10|\n",
      "+----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|       occupation|count|\n",
      "+-----------------+-----+\n",
      "|   Prof-specialty| 6172|\n",
      "|     Craft-repair| 6112|\n",
      "|  Exec-managerial| 6086|\n",
      "|     Adm-clerical| 5611|\n",
      "|            Sales| 5504|\n",
      "|    Other-service| 4923|\n",
      "|Machine-op-inspct| 3022|\n",
      "|                ?| 2809|\n",
      "| Transport-moving| 2355|\n",
      "|Handlers-cleaners| 2072|\n",
      "|  Farming-fishing| 1490|\n",
      "|     Tech-support| 1446|\n",
      "|  Protective-serv|  983|\n",
      "|  Priv-house-serv|  242|\n",
      "|     Armed-Forces|   15|\n",
      "+-----------------+-----+\n",
      "\n",
      "+------------------+-----+\n",
      "|    native_country|count|\n",
      "+------------------+-----+\n",
      "|     United-States|43832|\n",
      "|            Mexico|  951|\n",
      "|                 ?|  857|\n",
      "|       Philippines|  295|\n",
      "|           Germany|  206|\n",
      "|       Puerto-Rico|  184|\n",
      "|            Canada|  182|\n",
      "|       El-Salvador|  155|\n",
      "|             India|  151|\n",
      "|              Cuba|  138|\n",
      "|           England|  127|\n",
      "|             China|  122|\n",
      "|             South|  115|\n",
      "|           Jamaica|  106|\n",
      "|             Italy|  105|\n",
      "|Dominican-Republic|  103|\n",
      "|             Japan|   92|\n",
      "|         Guatemala|   88|\n",
      "|            Poland|   87|\n",
      "|           Vietnam|   86|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the most commonly used vals.\n",
    "dfraw.groupBy(dfraw[\"workclass\"]).count().orderBy(\"count\",ascending=False).show()\n",
    "dfraw.groupBy(dfraw[\"occupation\"]).count().orderBy(\"count\",ascending=False).show()\n",
    "dfraw.groupBy(dfraw[\"native_country\"]).count().orderBy(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing data imputation - Impute the most common row for \"?\".\n",
    "dfrawrp = dfraw.replace([\"?\"], [\"Private\"], [\"workclass\"])\n",
    "dfrawrpl = dfrawrp.replace([\"?\"], [\"Prof-specialty\"], [\"occupation\"])\n",
    "dfrawnona = dfrawrpl.replace([\"?\"], [\"United-States\"], [\"native_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+------------+--------+--------------+------+--------------------+--------------+-----------------+------------------+-------------+------+----------------+\n",
      "| age|capital_gain|capital_loss|   education|  fnlwgt|hours_per_week|income|      marital_status|native_country|       occupation|              race| relationship|   sex|       workclass|\n",
      "+----+------------+------------+------------+--------+--------------+------+--------------------+--------------+-----------------+------------------+-------------+------+----------------+\n",
      "|39.0|      2174.0|         0.0|   Bachelors| 77516.0|          40.0| <=50K|       Never-married| United-States|     Adm-clerical|             White|Not-in-family|  Male|       State-gov|\n",
      "|50.0|         0.0|         0.0|   Bachelors| 83311.0|          13.0| <=50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|      Husband|  Male|Self-emp-not-inc|\n",
      "|38.0|         0.0|         0.0|     HS-grad|215646.0|          40.0| <=50K|            Divorced| United-States|Handlers-cleaners|             White|Not-in-family|  Male|         Private|\n",
      "|53.0|         0.0|         0.0|        11th|234721.0|          40.0| <=50K|  Married-civ-spouse| United-States|Handlers-cleaners|             Black|      Husband|  Male|         Private|\n",
      "|28.0|         0.0|         0.0|   Bachelors|338409.0|          40.0| <=50K|  Married-civ-spouse|          Cuba|   Prof-specialty|             Black|         Wife|Female|         Private|\n",
      "|37.0|         0.0|         0.0|     Masters|284582.0|          40.0| <=50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|         Wife|Female|         Private|\n",
      "|49.0|         0.0|         0.0|         9th|160187.0|          16.0| <=50K|Married-spouse-ab...|       Jamaica|    Other-service|             Black|Not-in-family|Female|         Private|\n",
      "|52.0|         0.0|         0.0|     HS-grad|209642.0|          45.0|  >50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|      Husband|  Male|Self-emp-not-inc|\n",
      "|31.0|     14084.0|         0.0|     Masters| 45781.0|          50.0|  >50K|       Never-married| United-States|   Prof-specialty|             White|Not-in-family|Female|         Private|\n",
      "|42.0|      5178.0|         0.0|   Bachelors|159449.0|          40.0|  >50K|  Married-civ-spouse| United-States|  Exec-managerial|             White|      Husband|  Male|         Private|\n",
      "|37.0|         0.0|         0.0|Some-college|280464.0|          80.0|  >50K|  Married-civ-spouse| United-States|  Exec-managerial|             Black|      Husband|  Male|         Private|\n",
      "|30.0|         0.0|         0.0|   Bachelors|141297.0|          40.0|  >50K|  Married-civ-spouse|         India|   Prof-specialty|Asian-Pac-Islander|      Husband|  Male|       State-gov|\n",
      "|23.0|         0.0|         0.0|   Bachelors|122272.0|          30.0| <=50K|       Never-married| United-States|     Adm-clerical|             White|    Own-child|Female|         Private|\n",
      "|32.0|         0.0|         0.0|  Assoc-acdm|205019.0|          50.0| <=50K|       Never-married| United-States|            Sales|             Black|Not-in-family|  Male|         Private|\n",
      "|40.0|         0.0|         0.0|   Assoc-voc|121772.0|          40.0|  >50K|  Married-civ-spouse| United-States|     Craft-repair|Asian-Pac-Islander|      Husband|  Male|         Private|\n",
      "|34.0|         0.0|         0.0|     7th-8th|245487.0|          45.0| <=50K|  Married-civ-spouse|        Mexico| Transport-moving|Amer-Indian-Eskimo|      Husband|  Male|         Private|\n",
      "|25.0|         0.0|         0.0|     HS-grad|176756.0|          35.0| <=50K|       Never-married| United-States|  Farming-fishing|             White|    Own-child|  Male|Self-emp-not-inc|\n",
      "|32.0|         0.0|         0.0|     HS-grad|186824.0|          40.0| <=50K|       Never-married| United-States|Machine-op-inspct|             White|    Unmarried|  Male|         Private|\n",
      "|38.0|         0.0|         0.0|        11th| 28887.0|          50.0| <=50K|  Married-civ-spouse| United-States|            Sales|             White|      Husband|  Male|         Private|\n",
      "|43.0|         0.0|         0.0|     Masters|292175.0|          45.0|  >50K|            Divorced| United-States|  Exec-managerial|             White|    Unmarried|Female|Self-emp-not-inc|\n",
      "+----+------------+------------+------------+--------+--------------+------+--------------------+--------------+-----------------+------------------+-------------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfrawnona.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert strings to categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "dfnumeric = indexStringColumns(dfrawnona, [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+--------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "| age|capital_gain|capital_loss|  fnlwgt|hours_per_week|workclass|education|marital_status|occupation|relationship|race|sex|native_country|income|\n",
      "+----+------------+------------+--------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "|39.0|      2174.0|         0.0| 77516.0|          40.0|      3.0|      2.0|           1.0|       3.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|50.0|         0.0|         0.0| 83311.0|          13.0|      1.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0|         0.0|         0.0|215646.0|          40.0|      0.0|      0.0|           2.0|       8.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|53.0|         0.0|         0.0|234721.0|          40.0|      0.0|      5.0|           0.0|       8.0|         0.0| 1.0|0.0|           0.0|   0.0|\n",
      "|28.0|         0.0|         0.0|338409.0|          40.0|      0.0|      2.0|           0.0|       0.0|         4.0| 1.0|1.0|           8.0|   0.0|\n",
      "|37.0|         0.0|         0.0|284582.0|          40.0|      0.0|      3.0|           0.0|       2.0|         4.0| 0.0|1.0|           0.0|   0.0|\n",
      "|49.0|         0.0|         0.0|160187.0|          16.0|      0.0|     10.0|           5.0|       5.0|         1.0| 1.0|1.0|          12.0|   0.0|\n",
      "|52.0|         0.0|         0.0|209642.0|          45.0|      1.0|      0.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   1.0|\n",
      "|31.0|     14084.0|         0.0| 45781.0|          50.0|      0.0|      3.0|           1.0|       0.0|         1.0| 0.0|1.0|           0.0|   1.0|\n",
      "|42.0|      5178.0|         0.0|159449.0|          40.0|      0.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   1.0|\n",
      "|37.0|         0.0|         0.0|280464.0|          80.0|      0.0|      1.0|           0.0|       2.0|         0.0| 1.0|0.0|           0.0|   1.0|\n",
      "|30.0|         0.0|         0.0|141297.0|          40.0|      3.0|      2.0|           0.0|       0.0|         0.0| 2.0|0.0|           7.0|   1.0|\n",
      "|23.0|         0.0|         0.0|122272.0|          30.0|      0.0|      2.0|           1.0|       3.0|         2.0| 0.0|1.0|           0.0|   0.0|\n",
      "|32.0|         0.0|         0.0|205019.0|          50.0|      0.0|      6.0|           1.0|       4.0|         1.0| 1.0|0.0|           0.0|   0.0|\n",
      "|40.0|         0.0|         0.0|121772.0|          40.0|      0.0|      4.0|           0.0|       1.0|         0.0| 2.0|0.0|           0.0|   1.0|\n",
      "|34.0|         0.0|         0.0|245487.0|          45.0|      0.0|      8.0|           0.0|       7.0|         0.0| 3.0|0.0|           1.0|   0.0|\n",
      "|25.0|         0.0|         0.0|176756.0|          35.0|      1.0|      0.0|           1.0|       9.0|         2.0| 0.0|0.0|           0.0|   0.0|\n",
      "|32.0|         0.0|         0.0|186824.0|          40.0|      0.0|      0.0|           1.0|       6.0|         3.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0|         0.0|         0.0| 28887.0|          50.0|      0.0|      5.0|           0.0|       4.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|43.0|         0.0|         0.0|292175.0|          45.0|      1.0|      3.0|           2.0|       2.0|         3.0| 0.0|1.0|           0.0|   1.0|\n",
      "+----+------------+------------+--------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnumeric.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"native_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+--------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "| age|capital_gain|capital_loss|  fnlwgt|hours_per_week|sex|income|    workclass|      education|marital_status|    occupation| relationship|         race| native_country|\n",
      "+----+------------+------------+--------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "|39.0|      2174.0|         0.0| 77516.0|          40.0|0.0|   0.0|(8,[3],[1.0])| (16,[2],[1.0])| (7,[1],[1.0])|(14,[3],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|50.0|         0.0|         0.0| 83311.0|          13.0|0.0|   0.0|(8,[1],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|38.0|         0.0|         0.0|215646.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[0],[1.0])| (7,[2],[1.0])|(14,[8],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|53.0|         0.0|         0.0|234721.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[5],[1.0])| (7,[0],[1.0])|(14,[8],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|28.0|         0.0|         0.0|338409.0|          40.0|1.0|   0.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[0],[1.0])|(6,[4],[1.0])|(5,[1],[1.0])| (41,[8],[1.0])|\n",
      "|37.0|         0.0|         0.0|284582.0|          40.0|1.0|   0.0|(8,[0],[1.0])| (16,[3],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[4],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|49.0|         0.0|         0.0|160187.0|          16.0|1.0|   0.0|(8,[0],[1.0])|(16,[10],[1.0])| (7,[5],[1.0])|(14,[5],[1.0])|(6,[1],[1.0])|(5,[1],[1.0])|(41,[12],[1.0])|\n",
      "|52.0|         0.0|         0.0|209642.0|          45.0|0.0|   1.0|(8,[1],[1.0])| (16,[0],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|31.0|     14084.0|         0.0| 45781.0|          50.0|1.0|   1.0|(8,[0],[1.0])| (16,[3],[1.0])| (7,[1],[1.0])|(14,[0],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|42.0|      5178.0|         0.0|159449.0|          40.0|0.0|   1.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|37.0|         0.0|         0.0|280464.0|          80.0|0.0|   1.0|(8,[0],[1.0])| (16,[1],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|30.0|         0.0|         0.0|141297.0|          40.0|0.0|   1.0|(8,[3],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[0],[1.0])|(6,[0],[1.0])|(5,[2],[1.0])| (41,[7],[1.0])|\n",
      "|23.0|         0.0|         0.0|122272.0|          30.0|1.0|   0.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[1],[1.0])|(14,[3],[1.0])|(6,[2],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|32.0|         0.0|         0.0|205019.0|          50.0|0.0|   0.0|(8,[0],[1.0])| (16,[6],[1.0])| (7,[1],[1.0])|(14,[4],[1.0])|(6,[1],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|40.0|         0.0|         0.0|121772.0|          40.0|0.0|   1.0|(8,[0],[1.0])| (16,[4],[1.0])| (7,[0],[1.0])|(14,[1],[1.0])|(6,[0],[1.0])|(5,[2],[1.0])| (41,[0],[1.0])|\n",
      "|34.0|         0.0|         0.0|245487.0|          45.0|0.0|   0.0|(8,[0],[1.0])| (16,[8],[1.0])| (7,[0],[1.0])|(14,[7],[1.0])|(6,[0],[1.0])|(5,[3],[1.0])| (41,[1],[1.0])|\n",
      "|25.0|         0.0|         0.0|176756.0|          35.0|0.0|   0.0|(8,[1],[1.0])| (16,[0],[1.0])| (7,[1],[1.0])|(14,[9],[1.0])|(6,[2],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|32.0|         0.0|         0.0|186824.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[0],[1.0])| (7,[1],[1.0])|(14,[6],[1.0])|(6,[3],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|38.0|         0.0|         0.0| 28887.0|          50.0|0.0|   0.0|(8,[0],[1.0])| (16,[5],[1.0])| (7,[0],[1.0])|(14,[4],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|43.0|         0.0|         0.0|292175.0|          45.0|1.0|   1.0|(8,[1],[1.0])| (16,[3],[1.0])| (7,[2],[1.0])|(14,[2],[1.0])|(6,[3],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "+----+------------+------------+--------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols=[\"age\",\"capital_gain\",\"capital_loss\",\"fnlwgt\",\"hours_per_week\",\"sex\",\"workclass\",\"education\",\"marital_status\",\"occupation\",\"relationship\",\"native_country\",\"race\"]\n",
    "\n",
    "#VectorAssembler takes a number of collumn names(inputCols) and output column name (outputCol)\n",
    "#and transforms a DataFrame to assemble the values in inputCols into one single vector with outputCol.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(dfhot).select(\"features\", \"income\").withColumnRenamed(\"income\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(103, {0: 39.0, 1: 2174.0, 3: 77516.0, 4: 40.0, 9: 1.0, 16: 1.0, 31: 1.0, 40: 1.0, 52: 1.0, 57: 1.0, 98: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(103, {0: 50.0, 3: 83311.0, 4: 13.0, 7: 1.0, 16: 1.0, 30: 1.0, 39: 1.0, 51: 1.0, 57: 1.0, 98: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(103, {0: 38.0, 3: 215646.0, 4: 40.0, 6: 1.0, 14: 1.0, 32: 1.0, 45: 1.0, 52: 1.0, 57: 1.0, 98: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(103, {0: 53.0, 3: 234721.0, 4: 40.0, 6: 1.0, 19: 1.0, 30: 1.0, 45: 1.0, 51: 1.0, 57: 1.0, 99: 1.0}), label=0.0),\n",
       " Row(features=SparseVector(103, {0: 28.0, 3: 338409.0, 4: 40.0, 5: 1.0, 6: 1.0, 16: 1.0, 30: 1.0, 37: 1.0, 55: 1.0, 65: 1.0, 99: 1.0}), label=0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpoints.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(103,[0,1,3,4,9,1...|  0.0|\n",
      "|(103,[0,3,4,7,16,...|  0.0|\n",
      "|(103,[0,3,4,6,14,...|  0.0|\n",
      "|(103,[0,3,4,6,19,...|  0.0|\n",
      "|(103,[0,3,4,5,6,1...|  0.0|\n",
      "|(103,[0,3,4,5,6,1...|  0.0|\n",
      "|(103,[0,3,4,5,6,2...|  0.0|\n",
      "|(103,[0,3,4,7,14,...|  1.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|\n",
      "|(103,[0,1,3,4,6,1...|  1.0|\n",
      "|(103,[0,3,4,6,15,...|  1.0|\n",
      "|(103,[0,3,4,9,16,...|  1.0|\n",
      "|(103,[0,3,4,5,6,1...|  0.0|\n",
      "|(103,[0,3,4,6,20,...|  0.0|\n",
      "|(103,[0,3,4,6,18,...|  1.0|\n",
      "|(103,[0,3,4,6,22,...|  0.0|\n",
      "|(103,[0,3,4,7,14,...|  0.0|\n",
      "|(103,[0,3,4,6,14,...|  0.0|\n",
      "|(103,[0,3,4,6,19,...|  0.0|\n",
      "|(103,[0,3,4,5,7,1...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into training and testing sets.\n",
    "splits = lpoints.randomSplit([0.8, 0.2])\n",
    "\n",
    "#cache() : the algorithm is interative and training and data sets are going to be reused many times.\n",
    "adulttrain = splits[0].cache()\n",
    "adultvalid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adulttrain.write.saveAsTable(\"adulttrain\")\n",
    "adultvalid.write.saveAsTable(\"adultvalid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(adulttrain)\n",
    "#The above lines are same as..\n",
    "#lr = LogisticRegression()\n",
    "#lrmodel = lr.setParams(regParam=0.01, maxIter=1000, fitIntercept=True).fit(adulttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02018527662862479,0.00014267788604173205,0.0005512681747893359,5.51697040018831e-07,0.026724362791813904,-0.5319625312361737,0.02876629258403501,-0.38061299736152965,0.05456768119420952,-0.15246517592629183,0.28049078189096993,0.5622322239004728,-0.524830721960051,-1.3359465678348355,-0.34130330863613356,-0.004701469175193283,0.7651219397366995,1.0810252027160827,0.1696387773229618,-0.947919971842666,0.20446082350113265,-1.12618759629697,-1.4901165222293469,1.629269607379185,-1.1266486205478683,-0.6522324351446221,1.5972707343276396,-1.3272959707993568,-1.5067044467674686,-1.7670592686059454,0.8386927301458394,-0.6828800100827963,-0.2906199732505725,-0.37681924561378244,-0.29240191660699033,-0.23036839969460288,0.7616622656062908,0.21622565294068488,0.01771396049387079,0.6835759127697387,-0.0838933680168185,0.21667926434585005,-0.7349458019026479,-0.2976153009914244,-0.13370717954452432,-0.5682592181676618,-0.8127824116371712,0.47004305054148515,0.36029660347441517,-0.9718571550537287,0.09034475277178945,0.4308239361035779,-0.09928242558734053,-0.7813073334276871,-0.28261582353120007,1.3107463862430906,-0.4904410051445204,0.19545670631161136,-0.5808796117046441,0.45010355669810614,0.2843155940844825,-0.26922206094238593,0.3829885884990264,-0.31436135812697696,-0.058129351869298455,0.06422958541535742,0.5720011946808871,-0.5196281017660499,-0.6267045148264351,0.1734544588358208,0.49254322580835297,-0.8452912335163834,0.0008587202015741146,-0.5070630234386655,-0.2211394508820918,-0.9919227483650869,-1.2966420290144645,-0.24294028226617448,0.3392427642030794,0.05441078215303733,0.03861870925692606,0.1986583918328399,-0.31316218238737653,-0.8248974545904317,-0.0938447899011108,0.5504721531177806,1.1299783262996255,-0.6457871160989526,-0.4145941546447127,0.8541046533492366,-0.8457971813903234,0.6152468377390957,-1.0142838000048642,-0.3389936605815147,-0.6294238809287366,-0.09556480519560522,0.25140925922044166,0.0,0.08675273455172447,-0.15395453551071742,0.2104329377346878,-0.43875749638255807,0.07593385693617513]\n",
      "-4.287986635670084\n"
     ]
    }
   ],
   "source": [
    "#Interpret the model parameters\n",
    "print(lrmodel.coefficients)\n",
    "print(lrmodel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[0.65273620039858...|[0.65762679401211...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[0.75583497274098...|[0.68044877904877...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[3.69023178049778...|[0.97564191450204...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[5.22290469954717...|[0.99463726869528...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[3.41467495245978...|[0.96816002930221...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[4.08134172762075...|[0.98339556681037...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[1.40144728386494...|[0.80241345021242...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.35489014698306...|[0.91332213839078...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[3.10411230831667...|[0.95706205539186...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.68690644001921...|[0.93624958752275...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[0.08554463508294...|[0.52137312652069...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-10.768871002405...|[2.10440637088063...|       1.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.36371945271235...|[0.91401856268203...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-2.5170682627240...|[0.07467025999396...|       1.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[2.62361678860001...|[0.93236613635545...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[1.96000727222452...|[0.87653373945410...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[1.88208170928704...|[0.86785005294730...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[0.49442079667770...|[0.62114730470335...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  0.0|[1.70954866096847...|[0.84677773432555...|       0.0|\n",
      "|(103,[0,1,3,4,5,6...|  1.0|[-0.0028092908066...|[0.49929767776023...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate models using test dataset.\n",
    "#First, transform the validation set.\n",
    "validpredicts = lrmodel.transform(adultvalid)\n",
    "validpredicts.show()\n",
    "\n",
    "#rawPrediction : includes two values - log-odds that a sample doesn't and does belong to the category (making > 50,000).\n",
    "#probability : the probability that the sample is not in the category.\n",
    "#prediction : proability that the sample belongs to the category.\n",
    "#validpredicts.select(\"rawPrediction\").collect()\n",
    "#validpredicts.select(\"probability\").collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC:0.9024136346918922\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model. default metric : Area Under ROC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderPR:0.7591751098858374\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model. metric : Area Under PR\n",
    "bceval.setMetricName(\"areaUnderPR\")\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-fold validation and the results.\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "cv = CrossValidator().setEstimator(lr).setEvaluator(bceval).setNumFolds(5)\n",
    "#ParamGridBuilder() – combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000]).addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()\n",
    "#setEstimatorParamMaps() takes ParamGridBuilder().\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(adulttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.022456569848210317,0.00030947667004765635,0.0006466088616701111,6.32798180288299e-07,0.030109477578081186,-0.718596958410882,-0.4130490772382551,-0.9087994927142261,-0.4143616378855607,-0.6380007251813382,-0.2150081224290168,0.1511067399810792,-1.0398320556929355,-4.79751327710646,-0.58811178840551,-0.20758049092743244,0.619720382470168,0.9656710824164961,-0.04531956700907306,-1.3995379911831927,0.016078975278237533,-1.571974722687792,-2.0206842923389075,1.5982005363172302,-1.5354454615882882,-1.0037651170654671,1.565355684816618,-1.8728922151435476,-2.3010886508202657,-5.9122772885480215,1.2974579667465478,-1.4607890494371756,-1.0159799832762715,-1.1261096256402234,-1.004267853036903,-0.9519211363891856,1.2639893326906986,-0.0037968571196130988,-0.17147586231410616,0.5205675226645546,-0.28698523320134167,0.02689715853557866,-1.0850181624859174,-0.5109842845569028,-0.32052956733852306,-0.835586020928271,-1.129396451028462,0.31500099458533304,0.22277611528504668,-2.108860467442049,-0.0524307864261412,-0.4456859810010664,0.15799138345413727,-0.9897360362946981,0.019074543576659053,0.705969798454836,-0.8035898625146669,-0.9840956717114585,-1.8142023789361486,-0.7257802181392993,-0.9028055445167387,-1.4644622912719043,-0.7286198889701581,-1.583324116147348,-1.4489527755241012,-1.069422677012899,-0.6032487123184985,-1.9338132507916168,-2.0002283876985274,-0.8893736830387511,-0.6086408441857043,-2.1484900278447263,-1.338115817489248,-1.8971195597236687,-1.3820337798500042,-2.5933226260963917,-2.87120557271925,-1.4106060040464465,-0.6209711368468983,-1.2248137553878617,-1.161327294020821,-1.1361874025082177,-1.4223523703906853,-2.1140048210550892,-1.2914231307249406,-0.6167286590810713,0.09493471249372613,-2.0431251407940167,-1.711723498196027,-0.24572909552980446,-2.1818456917802163,-0.5159644856206266,-2.4481630605059688,-1.3891017380903499,-1.8246026005298488,-1.1228131231229643,-0.901923161269156,0.0,-0.6804134884287714,-0.9290525940235249,-0.4436892138331428,-1.3263591529029377,-0.6175279283293261]\n",
      "-1.3900913536404365\n",
      "1000\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print(cvmodel.bestModel.coefficients)\n",
    "print(cvmodel.bestModel.intercept)\n",
    "print(cvmodel.bestModel._java_obj.getMaxIter())\n",
    "print(cvmodel.bestModel._java_obj.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044081052518386"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7698303355382903"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904408105251841"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
